\documentclass[11pt]{article} % For LaTeX2e
\usepackage{nips12submit_e,times}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09

%\usepackage[usenames,dvipsnames]{color}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[scaled=0.8]{beramono}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{amsmath,amssymb}
\usepackage{array}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{placeins}

\usepackage{cite}
\usepackage{url}
\usepackage[hyperfootnotes=false,citecolor=RedOrange,linkcolor=RoyalBlue,urlcolor=DarkOrchid,colorlinks]{hyperref}
\usepackage[all]{hypcap}

\newcommand{\todo}[1]{\noindent\texttt{\color[rgb]{0.5,0.1,0.1} TODO: #1}}


\newcommand{\sectref}[1]{\hyperref[#1]{\mbox{Section~\ref*{#1}}}}
\newcommand{\figref}[1]{\hyperref[#1]{\mbox{Figure~\ref*{#1}}}}
\newcommand{\tabref}[1]{\hyperref[#1]{\mbox{Table~\ref*{#1}}}}
\newcommand{\eqtref}[1]{\hyperref[#1]{\mbox{Equation~\ref*{#1}}}}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1 & \mbox{if } #2 \\
			#3 & \mbox{if } #4
		\end{array}
	\right.
}

\title{Active Learning of Function Contours using Gaussian Process Confidence Bounds\thanks{Final
report for the course \emph{Research in Computer Science} (Spring
Semester 2012), carried out under the supervision of \emph{Prof. Andreas Krause.}}}

\author{
Alkis Gkotovos\\
Department of Computer Science, ETH Zurich\\
\texttt{alkisg@student.ethz.ch}
}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
We present a novel heuristic for actively identifying function
contours using Gaussian processes to model the underlying
function and utilizing the
inferred confidence bounds to guide the learning process. We also show how
the proposed heuristic can be extended to identify $\epsilon$-optimal function
contours. Our method is evaluated and compared to the state of the art on
synthetic and real-world seismographic data.
\end{abstract}

\section{Introduction}
In certain applications it is required to experimentally determine the regions
where the values of a function are above (or below) a certain threshold.
Furthermore, obtaining measurements of the function is expensive, so it is
important to
accurately determine the desired contours with as few samples as possible.
This suggests using an active learning procedure that selects measurements
over the function domain in a more efficient way than random sampling.

We propose a new heuristic for selecting measurements in the active learning
setting, which is based on
determining confidence bounds for the desired contours using a Gaussian
process to model the target function.
In~\sectref{sect:algo} we formalize the contour detection problem via active
learning,
review the previously proposed \emph{straddle}~\cite{bryan2005} heuristic,
and introduce the confidence bound heuristic, as well as its extension
to the $\epsilon$-optimal setting. In~\sectref{sect:exp} we
evaluate our heuristic on a synthetic function and a real-world seismographic
dataset.

\section{Algorithm} \label{sect:algo}
The problem of actively learning function contours can be described as follows.
Let ${f : \mathcal{D} \to \mathbb{R}}$ be the target function, where
$\mathcal{D}$ is a bounded subset of $\mathbb{R}^n$. We are interested in
finding the subset of $\mathcal{D}$ where this function takes values greater
than a specified threshold $t$, i.e. the set
$\mathcal{G}_t = \{x \in \mathcal{D} \mid f(x) > t\}$. To this end, we make
consecutive measurements $(x^{(i)}, f(x^{(i)}))$ of the function
and use them to determine $\mathcal{G}_t$. After the $i$-th step, the sets of
measurement positions and values are
$\mathcal{X}^{(i)} = \{x^{(1)},\ldots,x^{(i)}\}$ and
$\mathcal{Y}^{(i)} = \{f(x^{(1)}),\ldots,f(x^{(i)})\}$ respectively.

As mentioned before, our basic premise is that the effort required to obtain
each measurement is
non-negligible and, therefore, making a large number of random measurements
over the whole domain $\mathcal{D}$ is prohibitive. We rather aim at improving
the sampling process so that the chosen points $x^{(i)}$ are informative with
respect to the desired contours, which should result in accurately determining
the set $\mathcal{G}_t$ using a significantly smaller number of measurements
compared to random sampling.

In this setting, we have to decide on (1) a model that allows us to use the
measurements in order to obtain an approximation of $\mathcal{G}_t$, and (2) a
strategy to select the next point of sampling at each step. The following
subsections deal with these two issues.

\subsection{Gaussian processes}
We use Gaussian processes~\cite{gpbook} for modeling the target
function $f$, as they have been successfully used before in the context of
contour detection~\cite{bryan2005, bryan2008}. Gaussian process regression
offers a non-parametric framework for predicting the values of the function at
unobserved points given a number of measurements. The characteristics of
the assumed underlying model are primarily specified by a kernel function
${K : \mathcal{D} \times \mathcal{D} \to \mathbb{R}}$ and its hyperparameters.

At each step of active learning, $\mathcal{X}^{(i)}$ and $\mathcal{Y}^{(i)}$ are
used as training sets
for Gaussian process regression and the inferred mean $m^{(i)}(x)$ of the
model is used to approximate the set $\mathcal{G}_t$ as
${\mathcal{G}^{(i)}_t = \{x \in \mathcal{D} \mid m^{(i)}(x) > t\}}$.

\subsection{Choosing the next point}
The general strategy for choosing the next point of measurement at each step $i$
is to first establish a finite set $\mathcal{C}^{(i)} \subseteq \mathcal{D}$ of
candidate points, then evaluate some heuristic
${h : \mathcal{C}^{(i)} \to \mathbb{R}}$ for each candidate point and, finally,
choose the point with the largest score, that is
\begin{align*}
  x^{(i+1)} = \underset{x \in \mathcal{C}^{(i)}}{\argmax}\{h(x)\}
\end{align*}
If $\mathcal{D}$ is finite, we can use
$\mathcal{C}^{(i)} = \mathcal{D}\setminus\mathcal{X}^{(i)}$ or a random subset
thereof in case $\mathcal{C}^{(i)}$ is too large. Otherwise, we have to sample a set
of candidate points from $\mathcal{D}$. Random sampling is preferred over a
fixed grid to get better coverage of $\mathcal{D}$~\cite{bryan2005}.

Choosing the heuristic $h$ involves an exploration-exploitation trade-off. The
state of the art in contour detection combines the inferred variance of the
Gaussian process with the
distance from the inferred contour to create the straddle~\cite{bryan2005}
heuristic ${h_{straddle}(x) = 1.96\sigma(x) - | m(x) - t |}$, where $m(x)$
and $\sigma(x)$ are respectively the inferred mean and variance of the Gaussian
process at the candidate point $x$.

Our proposed heuristic is based on the fact that we can construct confidence
bounds for the desired contour by using the inferred mean and variance of
the Gaussian process. In particular, for every $x \in \mathcal{D}$ we obtain
a confidence interval ${[m(x) - \kappa\sigma(x), m(x) + \kappa\sigma(x)]}$,
where the probability of $f(x)$ lying in the interval depends on the value
of $\kappa$. Extending this idea and defining the functions
$u_{\kappa}(x) = m(x) + \kappa\sigma(x)$ and
$\ell_{\kappa}(x) = m(x) - \kappa\sigma(x)$,
we can create a confidence region for
the desired contour at level $t$ by using two sets
\begin{align*}
  \mathcal{S}_1 &= \{x \in \mathcal{D} \mid u_{\kappa}(x) \geq t\}\\
  \text{and}\hspace{1em}
  \mathcal{S}_2 &= \{x \in \mathcal{D} \mid \ell_{\kappa}(x) \leq t\}
\end{align*}
The confidence region is then
$\mathcal{S} = \mathcal{S}_1\cap\mathcal{S}_2$ (see~\figref{fig:cb}) and our
heuristic is defined
so that the point with the largest variance among the candidate points that
lie inside the confidence region is chosen:
\begin{align}\label{eq:hcb}
  h_{cb}(x) = \twopartdef { \sigma(x) } {x \in \mathcal{S}} {-\infty} {x \notin \mathcal{S}}
\end{align}
In other words, we exclude candidate points in the region where
$f(x) < t$ with high probability (the set $\mathcal{D}\setminus\mathcal{S}_1$)
and in the region where $f(x) > t$ with high probability
(the set $\mathcal{D}\setminus\mathcal{S}_2$).

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{figures/cb}
  \caption{A one-dimensional example showing the two sets $\mathcal{S}_1$ and
           $\mathcal{S}_2$, as well as the (shaded) confidence region
           $\mathcal{S} = \mathcal{S}_1\cap\mathcal{S}_2$ used by the confidence
           bound heuristic.}
  \label{fig:cb}
\end{figure}

From the above definitions it follows that 
$\mathcal{D}\setminus\mathcal{S}_2\subseteq\mathcal{S}_1$
and, equivalently, $\mathcal{S}\neq\varnothing$. We also expect that
obtaining more measurements will lead to lower variance values, which in turn
will generally reduce the size of $\mathcal{S}$.

\subsection{Detecting $\epsilon$-optimal contours}
The confidence bound heuristic described in the previous subsection can be
readily extended to detecting $\epsilon$-optimal function contours,
i.e. approximating the set
${\mathcal{G}_{\epsilon} = \{x \in \mathcal{D} \mid f(x) > \underset{x \in \mathcal{D}}{\max}\,f(x) - \epsilon\}}$.
In this case, the sets used to define the confidence region $\mathcal{S}$
are defined as
\begin{align*}
  \mathcal{S}_1 &= \{x \in \mathcal{D} \mid u_{\kappa}(x) \geq \underset{x \in \mathcal{D}}{\max}\,\ell_{\kappa}(x) - \epsilon\}\\
  \text{and}\hspace{1em}
  \mathcal{S}_2 &= \{x \in \mathcal{D} \mid \ell_{\kappa}(x) \leq \underset{x \in \mathcal{D}}{\max}\,u_{\kappa}(x) - \epsilon\}
\end{align*}
Using the same confidence region $\mathcal{S} = \mathcal{S}_1\cap\mathcal{S}_2$
as before, runs the risk of excluding areas where the true maximum might lie.
Since the approximation
${\mathcal{G}^{(i)}_\epsilon = \{x \in \mathcal{D} \mid m^{(i)}(x) > \underset{x \in \mathcal{D}}{\max}\,m^{(i)}(x) - \epsilon\}}$
of $\mathcal{G}_{\epsilon}$ at each step
depends on the inferred maximum, it is important to have an accurate estimate
of the true maximum in order to get an accurate approximation of the desired
$\epsilon$-optimal contour. For that reason, we include in $\mathcal{S}$ the
region where the true maximum could still lie with high probability, i.e.
the set
\begin{align*}
  \mathcal{S}_3 &= \{x \in \mathcal{D} \mid u_{\kappa}(x) \geq \underset{x \in \mathcal{D}}{\max}\,\ell_{\kappa}(x)\}
\end{align*}
Therefore, the resulting confidence region is
$\mathcal{S} = \mathcal{S}_1\cap\mathcal{S}_2\cup\mathcal{S}_3$, as shown
in~\figref{fig:cbe}, and the heuristic function is the same as
in~\eqtref{eq:hcb}.

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{figures/cb_eps}
  \caption{A one-dimensional example showing the three sets $\mathcal{S}_1$,
           $\mathcal{S}_2$ and $\mathcal{S}_3$, as well as
           the (shaded) confidence region
           $\mathcal{S} = \mathcal{S}_1\cap\mathcal{S}_2\cup\mathcal{S}_3$
           used by the $\epsilon$-optimal confidence bound heuristic.}
  \label{fig:cbe}
\end{figure}

\section{Experiments} \label{sect:exp}
We developed a contour detection framework prototype in Matlab, which uses the
GPML toolbox\footnote{\url{http://www.gaussianprocess.org/gpml/}} for
Gaussian process inference. In short, our framework allows for providing
targets as functions or discrete data sets,
implements the previously described heuristics for contour detection, and
provides contour evaluation and visualization capabilities.

In the following, we evaluate our proposed heuristic on two experiments.
The first is carried out on a synthetic two-dimensional sinusoidal target
function proposed by Bryan et al.~\cite{bryan2005} for evaluating the straddle
heuristic. The function is defined as
\begin{align*}
  f(x, y) = \sin(10x) + \cos(4y) - \cos(3xy)
\end{align*}
with domain
$\mathcal{D} = \{(x, y) \in \mathbb{R} \mid 0 \leq x \leq 1,\;0 \leq y \leq 2\}$.
For the second experiment we use a two-dimensional seismographic dataset
collected by 4089 sensors in California.
Before presenting the results, we first discuss two important issues that came up
during evaluation.

\noindent\emph{Kernel and hyperparameters.}\;The choice of the Gaussian process
kernel function and its hyperparameters can have a considerable impact on the
performance of contour detection. In both experiments we use a
Matern-5~\cite{gpbook} kernel function. For the first experiment, we initialize
the algorithm with 20 random measurements and optimize the hyperparameters with
respect to these measurements. Moreover, we re-optimize the hyperparameters at
exponentially increasing measurement-set sizes
(at $20\times 2^n$ samples, $n = 0, 1,\ldots$).
For the second experiment, we have another seismographic dataset
to our disposal, so we compute appropriate hyperparameters for this
dataset and use them in our experiment (without re-optimizing).

\noindent\emph{Evaluation metric.}\;We use three different ``metrics'' to
measure the quality of the inferred contours. The first metric is the
classification
accuracy of points being above or below the contour threshold. The second
metric uses also classification accuracy, but considers only points
outside the confidence region $\mathcal{S}$ as correctly classified.
We will call this metric \emph{strict accuracy}. Intuitively, it takes
into account
the certainty about the inferred contours according to the Gaussian process
model. The third metric measures topological similarity using a modified version
of the Hausdorff distance, which is normally defined as
$D_{hd}(\mathcal{X}, \mathcal{Y}) =
\max\{\underset{x\in\mathcal{X}}{\max}\;\underset{y\in\mathcal{Y}}{\min}\;d(x, y),
      \underset{x\in\mathcal{X}}{\max}\;\underset{y\in\mathcal{Y}}{\min}\;d(x, y)\}$,
where $d(x, y)$ is the Euclidean distance between points $x$ and $y$.
In our case, $\mathcal{X}$ and $\mathcal{Y}$ consist of a number of points on
the true and inferred contours respectively (as computed by Matlab's
\texttt{contourc}). Furthermore, to mitigate the effect of the inner maxima,
we replace them by $L^{20}$-norms of the corresponding sets converted to
vectors. Finally, we normalize the distance to lie in $[0, 1]$ and subtract it
from 1 in order to get a similarity score. We call the result
\emph{normalized Hausdorff similarity} metric or NHDS.

\subsection{Experiment 1}
In the first experiment we used a threshold value of $h = 0$, 1000 randomly
sampled candidates per step, and $\kappa = 2$.
\figref{fig:sin2d_steps} shows the evolution of the inferred contour and the
confidence region for a single execution of the algorithm using the confidence
bound heuristic. Note how the measurements gather around the true contour
as the confidence region shrinks. In \figref{fig:sin2d_eval} the performance
of our heuristic is compared to the straddle heuristic using the three metrics
presented before. The algorithm was run 20 times for each heuristic and the
errorbars have a height of two standard errors. We also depict the performance
of random sampling as a baseline. The performance of the two heuristics seems
to be comparable, while both are
clearly superior to random sampling. It is also evident that the NHDS metric
distinguishes in this case more clearly between
random sampling and the two heuristics than the other two metrics.

We also evaluate the $\epsilon$-optimal confidence bound heuristic on the same
function. The value of $\epsilon$ is chosen so that the desired contour
level be the same as above, i.e.
${\underset{x\in\mathcal{D}}{\max}\;f(x) - \epsilon = 0}$. The algorithm is
expected
to be less effective when searching for an $\epsilon$-optimal contour, since
it has to correctly infer an accurate maximum value in order to also infer
an accurate contour. Indeed, the results of~\figref{fig:sin2d_eval_eps} are
somewhat worse than the ones of~\figref{fig:sin2d_eval}.

\begin{figure}[tb]
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_20}
    \caption{20 measurements}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_40}
    \caption{40 measurements}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_80}
    \caption{80 measurements}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_160}
    \caption{160 measurements}
  \end{subfigure}
  \caption{Inferred contour (orange), true contour (dark gray) and confidence
           region $\mathcal{S}$ (light Gray) for one execution of the
           contour detection algorithm on the synthetic sinusoidal target
           function using our proposed confidence bound heuristic.}
  \label{fig:sin2d_steps}
\end{figure}

\begin{figure}[tb]
  \begin{subfigure}[b]{0.329\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_acc}
    \caption{Accuracy}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.329\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_strict_acc}
    \caption{Strict accuracy}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.329\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_hd}
    \caption{NHDS}
  \end{subfigure}
  \caption{Random, straddle and confidence bound heuristics evaluated on the three
           metrics for an increasing number of measurements
           (Experiment 1, 20 iterations).}
  \label{fig:sin2d_eval}
\end{figure}

\begin{figure}[tb]
  \begin{subfigure}[b]{0.329\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_eps_acc}
    \caption{Accuracy}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.329\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_eps_strict_acc}
    \caption{Strict accuracy}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.329\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sin2d_eps_hd}
    \caption{NHDS}
  \end{subfigure}
  \caption{Random and $\epsilon$-optimal confidence bound heuristics evaluated
           on the three metrics for an increasing number of measurements
           (Experiment 1, 20 iterations).}
  \label{fig:sin2d_eval_eps}
\end{figure}

\begin{figure}[h!]
  \begin{subfigure}[b]{0.329\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/quake_acc}
    \caption{Accuracy}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.329\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/quake_strict_acc}
    \caption{Strict accuracy}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.329\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/quake_hd}
    \caption{NHDS}
  \end{subfigure}
  \caption{Random, straddle and confidence bound heuristics evaluated on the three
           metrics for an increasing number of measurements
           (Experiment 2, 20 iterations).}
  \label{fig:quake_eval}
\end{figure}

\subsection{Experiment 2}
In contrast to the previous experiment, the seismographic data of our second
experiment consists of discrete data points and, thus, we have no ground
truth contours available. Therefore, we use the inferred model from all
available data samples as the ground truth wherever needed (for plotting and
for evaluating the NHDS). Furthermore, we use a threshold value of $h = 1$, every
unsampled data point is a candidate at each step, and $\kappa = 2$.

As before, we evaluate the performance of the algorithm using the three metrics
and the results are shown in \figref{fig:quake_eval}. It is noteworthy that
straddle seems to be equally effective to the confidence bound heuristic
according to accuracy, but is dramatically worse according to the other two
metrics. The reason for this behavior can be explained by referring to
\figref{fig:quake_expl}. In particular, we can see that the measurements of
the straddle heuristic are heavily concentrated on the inferred contour, while
the rest of the domain remains unexplored. As a consequence, the confidence region
does not decrease, which leads to worse strict accuracy scores, and the small
contour on the top right is not detected, which results in worse NHDS scores
(true contours far away from inferred contours are heavily penalized by the
NHDS metric). At the same time, the accuracy score is not affected by the
above observations, because it does not take into account the confidence
region and the effect of a few misclassified samples inside the small
undetected contour is negligible. As can be seen in the same figure,
the confidence bound heuristic explores enough to avoid these problems.

The importance of this difference between the two heuristics may depend on
the specific application. In any
case, we can conclude that the straddle heuristic exhibits more exploitative
behavior due to its preference to sample near already inferred contours, which
can lead to oversampling near already accurately determined contours. On the
other hand, the confidence bound heuristic is more exploratory and, in fact, is
similar to maximum variance sampling in cases where the confidence region
cannot be reduced (e.g. due to a bad choice of kernel hyperparameters).


\begin{figure}[tb]
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/quake_expl_straddle}
    \caption{Straddle heuristic}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/quake_expl_cb}
    \caption{Confidence bound heuristic}
  \end{subfigure}
  \caption{Example of inferred contours and confidence regions after 200
           measurements on the seismographic data of Experiment 2.}
  \label{fig:quake_expl}
\end{figure}

\section{Conclusion}
We presented a new heuristic for active learning of function contours, which is
based on the confidence bounds provided by Gaussian processes. A tentative
evaluation of the heuristic showed that it is comparable in performance to the
straddle heuristic and may perform better in some cases, depending on the
evaluation metric used. The issues of choosing an appropriate kernel and
a proper evaluation metric require further investigation. It would also be
interesting to evaluate the heuristic
(and its $\epsilon$-optimal extension) to higher-dimensional settings.

\bibliographystyle{plain}
\bibliography{report}

\end{document}
